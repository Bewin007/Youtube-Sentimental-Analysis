# -*- coding: utf-8 -*-
"""Youtube Comments Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Va6Okqx_Ayw6B_imPqjr4SItxVXelps3
"""

from googleapiclient.discovery import build
from textblob import TextBlob
import numpy as np

def new(limit,vid,video_id,youtube):
    YOUTUBE_API_SERVICE_NAME = "youtube"
    YOUTUBE_API_VERSION = "v3"
    youtube = build(YOUTUBE_API_SERVICE_NAME,YOUTUBE_API_VERSION,developerKey="AIzaSyABJ4Lnn15L56PON5KbDrWmhS00VY81gnc")
    ucom = []

    def load_comments(match):
        for item in match["items"]:

            comment = item["snippet"]["topLevelComment"]
            author = comment["snippet"]["authorDisplayName"]
            text = comment["snippet"]["textDisplay"]
            print("Comment by user {}: {}".format(author, text))
            ucom.append(text)

    def get_comment_threads(youtube, video_id, limit):
        results = youtube.commentThreads().list(
            part="snippet",
            maxResults=limit,
            videoId=video_id,
            textFormat="plainText"
        ).execute()
        return results

    def get_comment_thread(youtube, video_id, mytoken, limit):
        results = youtube.commentThreads().list(
            part="snippet",
            maxResults=limit,
            videoId=video_id,
            textFormat="plainText",
            pageToken=mytoken
        ).execute()
        return results


    limit1 = 100


    if limit>100:
        if limit%100==0:
            count=limit/100-1
        else:
            count=limit/100
    else:
        count=0
        limit1=limit
    
    match = get_comment_threads(youtube, video_id, limit1)
    next_page_token = match["nextPageToken"]
    load_comments(match)

    while count>0:
        if count==1:
            match1 = get_comment_thread(youtube, video_id, next_page_token, (limit-(limit/100)*100))
        else:    
             match1 = get_comment_thread(youtube, video_id, next_page_token, 100)
        next_page_token = match1["nextPageToken"]
        load_comments(match1)
        count=count-1

    print(len(ucom))


    import nltk 
    filtered_comments=[]
    import re
    def remove_emoji(string):
        emoji_pattern = re.compile("["
                            u"\U0001F600-\U0001F64F"  # emoticons
                            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                            u"\U0001F680-\U0001F6FF"  # transport & map symbols
                            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                            u"\U00002702-\U000027B0"
                            u"\U000024C2-\U0001F251"
                            "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', string)

    for comment in ucom:
        com = remove_emoji(comment)
        filtered_comments.append(com)
    print(filtered_comments)

    print(len(filtered_comments))

    import nltk
    import emoji
    import re 
    import statistics
    nltk.download('vader_lexicon')
    from nltk.sentiment.vader import SentimentIntensityAnalyzer
    sid=SentimentIntensityAnalyzer()

    positive = 0
    wpositive = 0
    spositive = 0
    negative = 0
    wnegative = 0
    snegative = 0
    neutral = 0
    track = []
    for comment in filtered_comments:
        i = sid.polarity_scores(comment)['compound']
        if (i == 0):  
                neutral += 1
        elif (i > 0 and i <= 0.3):
            wpositive += 1
        elif (i > 0.3 and i <= 0.6):
            positive += 1
        elif (i > 0.6 and i <= 1):
            spositive += 1
        elif (i > -0.3 and i <= 0):
            wnegative += 1
        elif (i > -0.6 and i <= -0.3):
            negative += 1
        elif (i > -1 and i <= -0.6):
            snegative += 1
        track.append(i)



    NoOfTerms = len(filtered_comments)


    positive = format(100 * float(positive) / float(NoOfTerms), '0.2f')
    wpositive = format(100 * float(wpositive) / float(NoOfTerms), '0.2f')
    spositive = format(100 * float(spositive) / float(NoOfTerms), '0.2f')
    negative = format(100 * float(negative) / float(NoOfTerms), '0.2f')
    wnegative = format(100 * float(wnegative) / float(NoOfTerms), '0.2f')
    snegative = format(100 * float(snegative) / float(NoOfTerms), '0.2f')
    neutral = format(100 * float(neutral) / float(NoOfTerms), '0.2f')



    Final_score = statistics.mean(track) 

    if Final_score>0:
        print("Using Vader Sentiment Analyzer: ")
        print("Overall Reviews are Positive with Score "+ str(format(100 * Final_score , '0.2f')+"% \n"))
    elif Final_score<0:
        print("Using Vader Sentiment Analyzer: \n")
        print("Overall Reviews are Negative with Score "+ str(format(100 * Final_score , '0.2f')+"% \n"))
    else :
        print("Using Vader Sentiment Analyzer: \n")
        print("Overall Reviews are Moderate with Score "+ str(format(100 * Final_score , '0.2f')+"% \n"))

    positive = 0
    wpositive = 0
    spositive = 0
    negative = 0
    wnegative = 0
    snegative = 0
    neutral = 0
    track = []
    for comment in filtered_comments:
        analysis = TextBlob(comment)
        i = analysis.sentiment.polarity
        if (i == 0):  
                neutral += 1
        elif (i > 0 and i <= 0.3):
            wpositive += 1
        elif (i > 0.3 and i <= 0.6):
            positive += 1
        elif (i > 0.6 and i <= 1):
            spositive += 1
        elif (i > -0.3 and i <= 0):
            wnegative += 1
        elif (i > -0.6 and i <= -0.3):
            negative += 1
        elif (i > -1 and i <= -0.6):
            snegative += 1
        track.append(i)



    NoOfTerms = len(filtered_comments)


    positive = format(100 * float(positive) / float(NoOfTerms), '0.2f')
    wpositive = format(100 * float(wpositive) / float(NoOfTerms), '0.2f')
    spositive = format(100 * float(spositive) / float(NoOfTerms), '0.2f')
    negative = format(100 * float(negative) / float(NoOfTerms), '0.2f')
    wnegative = format(100 * float(wnegative) / float(NoOfTerms), '0.2f')
    snegative = format(100 * float(snegative) / float(NoOfTerms), '0.2f')
    neutral = format(100 * float(neutral) / float(NoOfTerms), '0.2f')



    Final_score = statistics.mean(track) 

    if Final_score>0:
        print("Using TextBlob Sentiment Analyzer: ")
        print("    Overall Reviews are Positive with Score "+ str(format(100 * Final_score , '0.2f')+"% \n"))
    elif Final_score<0:
        print("Using TextBlob Sentiment Analyzer: \n")
        print("Overall Reviews are Negative with Score "+ str(format(100 * Final_score , '0.2f')+"% \n"))
    else :
        print("Using TextBlob Sentiment Analyzer: \n")
        print("Overall Reviews are Moderate with Score "+ str(format(100 * Final_score , '0.2f')+"% \n"))


    #pie chart


    y = np.array([positive, wpositive, spositive, negative,wnegative,snegative,neutral])
    mylabels = ['positive', 'weak positive', 'strong positive', 'negative','weak negative','strong negative','neutral']

    return(y)


def similar_video_main():
    #!/usr/bin/python
    # David Kohreidze
    # youtube-scraper.py

    # Library documentation
    # http://docs.python-requests.org/en/latest/user/quickstart/
    # http://www.crummy.com/software/BeautifulSoup/bs4/doc/


    import googleapiclient.discovery as discovery


    DEVELOPER_KEY = 'AIzaSyABJ4Lnn15L56PON5KbDrWmhS00VY81gnc'
    YOUTUBE_API_SERVICE_NAME = 'youtube'
    YOUTUBE_API_VERSION = 'v3'


    youtube = discovery.build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)
    video_response = youtube.videos().list(id=video_id, part='snippet,statistics,recordingDetails,contentDetails').execute()
    video_tags=video_response["items"][0]['snippet']['tags']

    import csv
    import re
    import requests
    import time
    from bs4 import BeautifulSoup

    # scrapes the title 
    def get_title():
        d = soup.find_all('h1', 'branded-page-header-title')
        for i in d:
            title = i.text.strip().replace('\n',' ').replace(',','').encode('utf-8') 
            f.write(title+',')
            print('\t%s') % (title)

    # scrapes the subscriber count
    def get_subs():
        b = soup.find_all('span', 'about-stat')
        for i in b:
            try:			
                value = i.b.text.strip().replace(',','')					
                if len(b) == 3:
                    f.write(value+',')
                    print('\t%s') %(value)
                elif len(b) == 2:
                    f.write('null,'+ value + ',')
                    print('\tsubs = null\n\t%s') %(value)
                else:
                    f.write('null,null,')
                    print('\tsubs = null\nviews = null')
            except AttributeError:
                pass

    # scrapes the description
    def get_description():
        c = soup.find_all('div', 'about-description')
        if c:
            for i in c:
                description = i.text.strip().replace('\n',' ').replace(',','').encode('utf-8')		
                f.write(description+',')
                print('\t%s') % (description)
                
                regex = re.compile(("([a-z0-9!#$%&'*+\/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+\/=?^_`"
                                    '{|}~-]+)*(@|\sat\s|\[at\])(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?(\.|'
                                    '\sdot\s))+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?)'))
                
                email = re.search(regex, description)
                if email:
                    if not email.group(0).startswith('//'):
                        print('\tEmail = ' + email.group())
                        f.write(email.group(0)+',')
                        k.write(email.group(0)+',')

                else:
                    print('\tEmail = null')
                    f.write('null,')
        else:
            print('\tDescription = null\n\tEmail = null')
            f.write('null,null,')

    # scrapes all the external links 
    def get_links():
        a = soup.find_all('a', 'about-channel-link ') # trailing space is required.
        for i in a:
            url = i.get('href')
            externalLinks.append(url)
            f.write(url+',')
            print('\t%s') % (url)

    # scrapes the related channels
    def get_related():
        s = soup.find_all('h3', 'yt-lockup-title')
        for i in s:
            t = i.find_all(href=re.compile('user'))
            for i in t:
                url = 'https://www.youtube.com'+i.get('href')
                related.write(url+'\n')
                #print('\t\t%s,%s') % (i.text, url)	

    # create output files
    f = open('meta-data.csv', 'w+')
    k = open('key-data.csv', 'w+')
    related = open('related-channels.csv', 'w+')

    # empy list to save pages we've already scraped
    visited = []

    externalLinks = []

    # disassemble YouTube search result page URL
    base = 'https://www.youtube.com/results?search_query='
    page = '&page='
    q = video_tags # enumerate all keywords here
    count = 1 # start on page 1
    pagesToScrape = 1 # the number of search result pages to scrape
    timeToSleep = 3 # the number of seconds between pings to the YouTube server

    # set outout csv file column labels
    f.write('url, title, subs, views, description, email, external links\n')

    for query in q:
        while count <= pagesToScrape:
            # assemble the URL to scrape
            scrapeURL = base + str(query) + page + str(count)
            
            print('\nScraping {} \n'.format(scrapeURL))
            
            # ping and retrieve search result page HTML 
            r = requests.get(scrapeURL)

            # create Soup object from HTML 
            soup = BeautifulSoup(r.text)

            # parse channel container
            users = soup.find_all('div', 'yt-lockup-byline')
            
            for each in users:
                # parse all URLs that contain 'user'
                a = each.find_all(href=re.compile('user'))
                for i in a:
                    # assemble channel's about page; this is where our data is located
                    url = 'https://www.youtube.com'+i.get('href')+'/about'
                    
                    # check to see if channel has already been scraped
                    if url in visited:
                        print('\t%s has already been scraped\n\n') %(url)
                    else:
                        # ping and retreive channel's HTML, store as Soup object
                        r = requests.get(url)
                        soup = BeautifulSoup(r.text)

                        # output channel url to csv file & terminal
                        f.write(url + ',')
                        k.write(url +',')
                        print('\t%s') %(url)

                        # scrape the meta data
                        get_title()
                        get_subs()
                        get_description()
                        get_links()
                        get_related()

                        # formatting csv & terminal output
                        f.write('\n')	
                        print('\n')

                        # add url to visited list
                        visited.append(url)

                        # time delay between pings to YouTube server
                        time.sleep(timeToSleep)
            
            # iterate to the next search result page
            count += 1
            print('\n')
        
        # reset page count as we've iterated to next search term
        count = 1
        print('\n')	

    print ("dwa"+externalLinks)
    f.close()
    k.close()
    return externalLinks